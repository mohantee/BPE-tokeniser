# ğŸ”¤ BPE Tokenizer - Hindi Text

A lightweight **Byte Pair Encoding (BPE)** tokenizer trained on Hindi text, featuring an interactive web UI built with Gradio (similar to Tiktokenizer).

## ğŸŒŸ Quick Start

### Try Online
ğŸ‘‰ **[Try the demo on Hugging Face Spaces](https://huggingface.co/spaces)** (coming soon!)

### Local Usage
```bash
# Install dependencies
pip install -r requirements.txt

# Train tokenizer (optional - if hindi_tokenizer.json doesn't exist)
python main.py

# Run the web UI
python app.py
```

Then open your browser to `http://localhost:7860` ğŸš€

## ğŸ“ How to Use

### Encoding (Text â†’ Tokens)
1. Go to the **Encode** tab
2. Paste Hindi text
3. Click **ğŸ” Encode**
4. See token IDs and compression statistics

### Decoding (Tokens â†’ Text)
1. Go to the **Decode** tab
2. Paste token IDs like `[256, 257, 258]` or `256, 257, 258`
3. Click **ğŸ”“ Decode**
4. Get back the original text

## ğŸ¨ Features

âœ… **Web Interface**
- Clean, intuitive Gradio UI (like Tiktokenizer)
- Encode and decode in real-time
- Copy-paste token IDs easily
- Pre-loaded example texts

ğŸ“Š **Statistics**
- Token count
- Compression ratio
- Byte-level analysis
- Input/output size comparison

ğŸ‡®ğŸ‡³ **Hindi Support**
- Handles Devanagari script perfectly
- UTF-8 encoding support
- Proper Unicode handling

## ğŸ“‚ Project Structure

```
BPE-tokeniser/
â”œâ”€â”€ app.py                    # ğŸ¨ Gradio web interface (main app)
â”œâ”€â”€ tokenizer.py             # ğŸ”§ BPE tokenizer implementation
â”œâ”€â”€ main.py                  # ğŸš‚ Training script
â”œâ”€â”€ hindi_tokenizer.json     # ğŸ“¦ Pre-trained tokenizer
â”œâ”€â”€ data.txt                 # ğŸ“š Training data
â”œâ”€â”€ requirements.txt         # ğŸ“‹ Dependencies
â”œâ”€â”€ README.md               # ğŸ“– This file
â”œâ”€â”€ DEPLOY.md               # ğŸš€ Deployment guide
â””â”€â”€ .gitignore              # ğŸ™ˆ Git ignore rules
```

## ğŸ”¬ Technical Details

### Algorithm: Byte Pair Encoding (BPE)

**What it does:**
- Starts with 256 byte-level tokens (0-255)
- Iteratively merges most frequent byte pairs
- Creates new token IDs (256+)
- Reaches desired vocabulary size (default: 6,000)

**Why BPE?**
- âœ… Handles any Unicode text
- âœ… Balances vocabulary size and sequence length
- âœ… Better compression than character-level tokenization
- âœ… Better context preservation than word-level tokenization

### Example Tokenization

```
Input Text:    "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤•à¥ˆà¤¸à¥‡ à¤¹à¥‹"
                     â†“
Pre-tokenize:  ["à¤¨à¤®à¤¸à¥à¤¤à¥‡", "à¤•à¥ˆà¤¸à¥‡", "à¤¹à¥‹"]
                     â†“
UTF-8 Bytes:   [224, 164, ..., 255, 224, 165, ..., 255, ...]
                     â†“
Apply BPE:     [512, 513, 514, 515]  â† Token IDs
                     â†“
Compression:   32 bytes â†’ 4 tokens (8X compression)
```

### Tokenizer Configuration

| Setting | Value |
|---------|-------|
| **Language** | Hindi (Devanagari) |
| **Vocabulary Size** | 6,000 tokens |
| **Base Tokens** | 256 (bytes) + 1 (separator) |
| **Training Data** | `data.txt` |
| **Output Format** | List of integers |

## ğŸš€ Deploy to Hugging Face Spaces

### Option 1: Using Hugging Face Web Interface
1. Go to [huggingface.co/spaces](https://huggingface.co/spaces)
2. Click **Create new Space**
3. Choose **Gradio** as SDK
4. Upload files from this repo
5. Done! Your app runs automatically

### Option 2: Using Git (Git LFS for large files)
```bash
# Clone your space repo
git clone https://huggingface.co/spaces/[username]/bpe-tokenizer

# Copy files
cp app.py tokenizer.py requirements.txt hindi_tokenizer.json /path/to/space/

# Push (with LFS for json file)
cd /path/to/space/
git lfs track "*.json"
git add .
git commit -m "Add BPE tokenizer"
git push
```

### Option 3: Docker Deployment
```bash
# Build Docker image
docker build -t bpe-tokenizer .

# Run container
docker run -p 7860:7860 bpe-tokenizer
```

See [DEPLOY.md](DEPLOY.md) for detailed deployment instructions.

## ğŸ› ï¸ Installation

### Requirements
- Python 3.7+
- pip or conda

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Dependencies
- **gradio** - Web UI framework
- **regex** - Advanced regex for Unicode handling

## ğŸ“š Usage Examples

### Example 1: Simple Greeting
```
Input:  "à¤¨à¤®à¤¸à¥à¤¤à¥‡"
Output: [256, 257, 258]
Stats:  12 bytes â†’ 3 tokens (4X compression)
```

### Example 2: Full Sentence
```
Input:  "à¤­à¤¾à¤°à¤¤ à¤•à¥€ à¤…à¤°à¥à¤¥à¤µà¥à¤¯à¤µà¤¸à¥à¤¥à¤¾ à¤¦à¥à¤¨à¤¿à¤¯à¤¾ à¤®à¥‡à¤‚ à¤¤à¥‡à¤œà¥€ à¤¸à¥‡ à¤µà¤¿à¤•à¤¸à¤¿à¤¤ à¤¹à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤"
Output: [256, 257, 258, 259, ...] (60+ tokens)
Stats:  112 bytes â†’ 60 tokens (1.9X compression)
```

## ğŸ”„ Training Your Own Tokenizer

### Using Your Data

1. Replace `data.txt` with your Hindi text
2. Update `main.py` if needed:
   ```python
   tokenizer.train(text, vocab_size=8000)  # Change vocab size if desired
   ```
3. Run training:
   ```bash
   python main.py
   ```
4. New `hindi_tokenizer.json` will be created

### Training Parameters
- `vocab_size`: Number of tokens (default: 6000, range: 256-50000)
- Larger vocab â†’ better compression but more memory
- Smaller vocab â†’ worse compression but faster inference

## ğŸ“– API Reference

### Initialize
```python
from tokenizer import HindiTokenizer

tokenizer = HindiTokenizer()
tokenizer.load("hindi_tokenizer.json")
```

### Encode Text to Tokens
```python
tokens = tokenizer.encode("à¤¨à¤®à¤¸à¥à¤¤à¥‡")
print(tokens)  # [256, 257, 258]
```

### Decode Tokens to Text
```python
text = tokenizer.decode([256, 257, 258])
print(text)  # "à¤¨à¤®à¤¸à¥à¤¤à¥‡"
```

### Save/Load Tokenizer
```python
# Save
tokenizer.save("my_tokenizer.json")

# Load
tokenizer.load("my_tokenizer.json")
```

## ğŸ¯ Tips & Tricks

### For Best Results
1. **Use UTF-8 encoding** - Ensure all input files are UTF-8
2. **Clean data** - Remove duplicates and irrelevant content
3. **Large dataset** - Use more training data for better tokenization
4. **Tune vocab size** - Test different sizes for your use case

### Troubleshooting
- **Tokenizer not loading?** - Check `hindi_tokenizer.json` exists
- **Wrong encoding?** - Ensure input is valid Hindi text
- **Memory issues?** - Reduce vocabulary size
- **Slow inference?** - Use smaller vocabulary

## ğŸ¤ Contributing

Want to improve this? 
- Add support for other languages
- Optimize tokenization speed
- Improve UI/UX
- Add more examples
- Better documentation

Feel free to fork and submit PRs!

## ğŸ“„ License

MIT License - Use freely!

## ğŸ™ Acknowledgments

- Based on [BPE algorithm](https://en.wikipedia.org/wiki/Byte_pair_encoding)
- UI inspired by [Tiktokenizer](https://tiktokenizer.vercel.app/)
- Built with [Gradio](https://www.gradio.app/) 
- Python's excellent [regex](https://github.com/mrabarnett/regex) library

---

## ğŸ“ Support

- ğŸ“– Check [DEPLOY.md](DEPLOY.md) for deployment help
- ğŸ› Open an issue for bugs
- ğŸ’¡ Suggestions welcome!
- ğŸ“§ Contact maintainer

---

Made with â¤ï¸ for Hindi NLP | Last updated: November 2025